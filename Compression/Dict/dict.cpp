/*

DICT - алгоритм словарной замены. Построение словаря осуществляется
       во время работы программы, использованный словарь выводится
       перед закодированными данными

Макросы, управляющие компиляцией:
PPMD_VERSION - создать версию программы, нацеленную на лучшее сжатие PPM-алгоритмами
DEBUG        - включить сбор статистики, печатаемой опциями -v
DICT_LIBRARY - не включать main() и прочие функции, необходимые только для standalone программы

To-do list:
 + эксперименты с GOOD_WORD
 + использовать редкие символы (правда "для exe, doc и т.п." это ни хрена не сработало :()
 + уменьшать требования к словам для "быстрого режима" (NB! сделать это параметризуемым)
 + статистика по словарю/кодированию
 + в 7 раз ускорен FindWord
 + исправлены ошибки типа пустого словаря
 + попробовать слова [a-z]+ - фигушки!
 + сканирование данных - использовать хеш с гарантией нахождения слова (как в кодировании)
 + сортировка словаря для увеличения его сжатия (отдельно 1- и 2-байтовых строк)
 + кодирование словаря: разделитель слов и сколько букв из последнего слова повторять
 + -DDICT_LIBRARY - не определять функцию main()
 - DictDecode() - использовать функции чтения/записи чтобы работать в фиксированном объёме памяти
 - предусмотреть выход если найдено слишком мало слов, слишком мало weak chars и в других пограничных случаях
 - настройка параметров отбора слов в зависимости от входных данных (ghc-src - VERY_GOOD_CNT=8000, rus - option_fast...)
 - кодирование словаря: концы слов
 - exe: we love any chars! ;)
 - сделать все переменные локальными и использовать malloc
 - использовать chr+' ' для кодирования украденного chr - impossible


АЛГОРИТМ РАБОТЫ

1. Пройти по файлу, создавая список слов:
   * хеш-таблица из 4-байтовых элементов отмечает счётчик слова (2 байта) и хеш его родителя (ещё 2 байта)
   * хеш родителя используется для тестирования коллизий, при коллизии делается до 13 рехеширований
   * для каждого слова смотрим его счётчик в таблице:
       счётчик=0 -> добавить слово в создаваемый список слов и счётчик:=1
       счётчик<5 -> счётчик++
       иначе     -> добавить к слову следующую букву из входного текста и повторить цикл
   * по выходу из цикла начинаем его заново со следующего двухбуквенного слова из входного текста

2. Пройти по списку слов и отдать счётчики однодетных слов их детям (эти слова легко опознать
   по их счётчику, имеющему минимальное возможное значение), исходя из предположения, что эти
   счётчики отмечают вхождения тех же детей до того, как было получено разрешение на их рождение.
   Затем пройти по списку слов с конца в начало (чтобы сначала обработать производные, более длинные
   слова) и отдать счётчики слишком редких слов их родителям. Счётчики извлекаются из хеш-таблицы,
   созданной на первом шаге

3. Теперь мы имеем список наиболее употребительных слов со счётчиками и частоты отдельных символов
   во входном тексте, которые нетрудно собрать между делом на первом этапе.
   Отсортировав оба списка по частотам, мы легко узнаем, какие символы можно задействовать для
   кодирования слов и какие слова будут кодироваться одно-, и какие двух-буквенными сокращениями

4. Присваиваем коды словам

5. Выводим словарь в выходной поток

6. Строим небольшой хеш для всех используемых слов и их частичных начал.
   Этот хеш должен позволить нам просканировать текст до конца слова и определить его код.
   Поскольку количество слов <10k и их суммарная длина <100k, то этот хеш должен полностью влезать
   в кеш процессора

7. Кодируем текст, используя этот хеш


КАК ПРЕДОТВРАТИТЬ ПОЯВЛЕНИЕ ФАНТОМНЫХ СЛОВ С ЧАСТОТОЙ 4?

1. Пройти по списку слов из начала в конец. если родитель имеет минимальную частоту,
   какую только может иметь слово с ребёнком, то это означает, что текущее слово - его
   единственный ребёнок и резонно предположить, что все вхождения родительского слова
   были на самом деле текущим словом. В этом случае забрать её себе с пометкой "фантомная частота".
   в результате все фантомные частоты должны будут "скатиться" к наиболее длинным словам из этой
   ветки (РЕАЛИЗОВАНО КАК ПЕРВАЯ ЧАСТЬ ЭТАПА 2)

2. Альтернативно - пройти второй раз по файлу, подсчитывая частоты уже известных слов,
   но не создавая новых


РАСПРЕДЕЛЕНИЕ СЛОВ МЕЖДУ ОДНОБАЙТОВЫМИ И ДВУХБАЙТОВЫМИ КОДАМИ

1. Давать однобайтовые коды наиболее частым словам, безотносительно к их длине (РЕАЛИЗОВАНО)
2. Отдавать словам коды слишком редких символов (РЕАЛИЗОВАНО)


ЗАПИСЫВАТЬ ТЕКСТ ОДНОБАЙТОВЫХ СЛОВ В НАЧАЛЕ ФАЙЛА, А ДВУХБАЙТОВЫХ - ПО МЕСТУ ИХ ПЕРВОГО УПОТРЕБЛЕНИЯ

0x11 0x12 abcde .... 0x11 0x12 (кодирует слово abcde). При этом _длины_ всех слов записывать в начале файла


ИСПОЛЬЗОВАТЬ ДЛЯ ПРОВЕРКИ СЛОВ (MIN_VISITS_TO_HAVE_SON, ALLOW_TO_EXTEND_WORD, GOOD_WORD)
ВЕРОЯТНОСТИ СИМВОЛОВ И БУКВОСОЧЕТАНИЙ, КОТОРЫЕ МОЖНО СОБРАТЬ В ПРЕДВАРИТЕЛЬНОМ ПРОХОДЕ

К примеру, ALLOW_TO_EXTEND_WORD(c1,c2) = True, если вероятность появления c2 после c1 < 10%
В общем, должен работать принцип антиконтекстного моделирования - запоминать то, что действительно необычно


УЛУЧШЕНИЕ СЖАТИЯ

1. Доп. проход для сбора реальной статистики (оптимизация словаря при смягчении критерия GOOD_WORD)
2. При формировании слова учитывать вероятности p0[-1], p0[0], p[-1], p[0] и отношение cnt/cnt0
     чтобы максимально избегать формирования слов типа "yteString"
3. При наличии среди Good_Words слова 'Message' вчетверо уменьшать count для слова 'essage'
     (при условии, что есть не более 2 слов типа '?essage'). Это позволит избавиться от тонн мусора
4. Для binary files: использовать больше символов (сейчас там критерий word_count>10*char_count) (РЕАЛИЗОВАНО)


УЛУЧШЕНИЕ СКОРОСТИ

1. Шагать на 4 байта за раз, начать поиск слова с 2 байтов (РЕАЛИЗОВАНО)
2. После коллизии искать совпадение сначала в пределах кеш-строки CPU (16/32 байта). Бессмысленно - количество хеш-коллизий не превышает 10-20%
3. Располагать "abc", "abcd", "abcde" в одной кеш-строке CPU и только по её исчерпании делать полноценный update_hash
4. Возвратить бинарный поиск, но делать его только после 1-2-4-байтового индексирования. Для того,
     чтобы сделать его точным, надо сравнивать входной текст с обоими словами, между которыми он
     оказался, и от более близкого по количеству букв идти назад по цепочке его префиксных слов.
     Отделить массив бинарного поиска от массива полной информации по словам чтобы позволить
     ему без мыла залезть в кеш-память (10к слов - 40кб, плюс 100кб их текста)


УМЕНЬШЕНИЕ РАСХОДА ПАМЯТИ

1. Количество создаваемых слов в 50-100 раз меньше размера файла. На каждое слово необходимо
     16 байт в массиве FirstWord и 4 байта в хеше scan_hash, но в последнем желательно
     иметь 4-кратный запас элементов для обеспечения высокой производительности.
     Итого выходит 32 байта на слово, что при кол-ве слов, равном 1/32 от объёма входных данных,
     займёт столько же памяти, сколько весь входной файл. Кроме того, на случай нетипичных файлов
     желательно иметь возможность создавать большее количество слов
2. Хеш-таблицу можно уменьшить вдвое (РЕАЛИЗОВАНО), структуру Word сократить до 9 байт (byte len, byte3 hash, byte hash0_high, cnt_t count)
     Итого получится 17 байт на слово (вместо 32), с 10-20% деградацией итоговой производительности
3. FirstWord - использовать список блоков вместо таблицы фиксированного размера
4. Если вставлять в хеш-таблицу только слова чётной (или кратной 4) длины,
     то можно вдвое-вчетверо сократить их количество

*/


// ЭВРИСТИКИ ДЛЯ ОТБОРА СТРОК **********************************************************************
// Количество повторов слова перед добавлением в словарь его "детей" (зависит от его длины)
#define MIN_VISITS_TO_HAVE_SON(len)  ((len)>10? 2 : 5)

// Философский вопрос - может ли слово, заканчиваюшееся символом c1, завести ребёнка от символа c2?
#define ALLOW_TO_EXTEND_WORD(c1,c2)  (char_class(c1)==char_class(c2))
// Для решения этого вопроса все символы разбиваются на классы (как в любом уважающем себя обществе).
// Каждое порядочное слово должно состоять только из символов, принадлежащих одному классу.
// Сейчас классов два: управляющие символы плюс пробел, и все остальные

// Оставлять при очистке словаря только слова, выигрыш от использования которых будет достаточно велик.
// Здесь cnt/len - счётчик/длина слова, cnt0 - счётчик его родителя
#define GOOD_WORD(cnt,cnt0,len)  (cnt>MinLargeCnt                                     \
                                  || (cnt0?  cnt>MinMediumCnt && cnt>cnt0*MinRatio    \
                                          :  cnt>MinSmallCnt))                        \

// Это слово годится для кодирования даже двумя байтами?
#define GOOD_2BYTE_WORD(len,cnt)  (len>=4)


// КОНСТАНТЫ АЛГОРИТМА ****************************************************************
// Условная длина однобайтовой строки для тех байтов, с которых начинаются двухбайтовые строки
#define USE_DICT2                    1
// Этот символ никогда не используется для кодов слов, вместо этого он трактуется специальным образом
#define RESERVED_CHAR                ' '
// Иакс. длина кодируемого слова
#define MAX_WORD_LEN                 254


// ХОЗЧАСТЬ ************************************************************************
#include <stdio.h>
#include <ctype.h>
#include <limits.h>

#include "../Compression.h"

typedef int            count_t;  // Счётчики слов

#ifdef PPMD_VERSION
// Полновесные счётчики, на 0.2% улучшают сжатие в PPMD
#define SCNT_MAX INT_MAX
typedef int      scnt_t;
typedef unsigned hash0_t;
#else
// Для экономии памяти и лучшего/более быстрого сжатия в lzma/ppmonstr
#define SCNT_MAX SHRT_MAX
typedef short  scnt_t;
typedef ushort hash0_t;
// Если кому-нибудь когда-нибудь понадобится версия, способная сжимать и для ppmd, и для lzma,
// то для неё эти определения должны выглядеть как:
//#define SCNT_MAX (option_ppmd? INT_MAX : SHRT_MAX)
//typedef int scnt_t;
#endif

#ifdef DEBUG
// Немного статистики
int FindWord_calls     // количество вызовов FindWord
  , OutByte            // количество выведенных байт
  , OutByte2           // количество байт, закодированных двумя байтами
  , OutWord            // количество выведенных слов
  , used_hash1[13+1]   // количество хеш-коллизий при парсинге
  , used_hash2[13+1]   // количество хеш-коллизий при кодировании
  , depth_cnt[MAX_WORD_LEN+2]      // количество поисков в хеше на глубине (длине слова) n
  , matrix_cnt[MAX_WORD_LEN+2][MAX_WORD_LEN+2]   // количество отходов из позиции i на j шагов назад
  , mc[MAX_WORD_LEN+2]                           // суммарные значения по столбцам matrix_cnt
  , increment_cnt[MAX_WORD_LEN+2]  // количество операций инкремента слишком малого счётчика для слов с разными длинами в phase1
  , addword_cnt[MAX_WORD_LEN+2]    // количество операций AddWord для слов с разными длинами в phase1
  , badword_cnt[MAX_WORD_LEN+2]    // количество инкрементов счтчика в bad_word для слов с разными длинами в phase1
  ;
#endif

#ifdef DICT_LIBRARY
#define stat1(nextmsg)
#define stat2(nextmsg)
#else
void stat1 (char *nextmsg);
void stat2 (char *nextmsg);
#endif


// ОПЦИИ КОМАНДНОЙ СТРОКИ **********************************************************************
// Объем информации, выдаваемой на stdout
//   0   только ошибки
//   1   суммарная статистика каждого шага процесса
//   2   информация только о выживших словах
//   3   информация о всех словах и каждом этапе их жизни
//   4   + информация о процессе кодирования
int verbose = 0;

// Печатать время выполнения каждого шага алгоритма
int print_timings = 0;

// Использовать для слов только однобайтовые коды
int use_plain_dictionary = 0;


#ifndef FREEARC_DECOMPRESS_ONLY


// СТРУКТУРЫ ДАННЫХ **********************************************************************
count_t char_counts[UCHAR_MAX+1];   // Счётчики встречаемости отдельных сисмволов в исходном тексте

// Разбить все символы на несколько классов. Слово должно состоять из символов только одного класса
char char_class_table[UCHAR_MAX+1] = {
    0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,  // 0-15
    0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,  // 16-31
#if 1
    0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,  //  !"#$%&'()*+,-./
    1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,  // 0123456789:;<=>?
    1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,  // @ABCDEFGHIJKLMNO
    1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,  // PQRSTUVWXYZ[\]^_
    1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,  // `abcdefghijklmno
    1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,  // pqrstuvwxyz{|}~
#else  // Альтернативный вариант, дает выигрыш на 4dos.doc и ДОЧЕРИ МОНТЕСУМЫ
    0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,  //  !"#$%&'()*+,-./
    1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,  // 0123456789:;<=>?
    0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,  // @ABCDEFGHIJKLMNO
    1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,  // PQRSTUVWXYZ[\]^_
    0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,  // `abcdefghijklmno
    1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,  // pqrstuvwxyz{|}~
#endif
#if 1
    1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,
    1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,
    1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,
    1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,
    1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,
    1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,
    1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,
    1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
#elif 0
    2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,
    2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,
    2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,
    2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,
    2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,
    2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,
    2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,
    2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2
#else
    2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,  // АБВГДЕЖЗИЙКЛМНОП      CP-866
    2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,  // РСТУФХЦЧШЩЪЫЬЭЮЯ
    2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,  // абвгдежзийклмноп
    0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,  // 176-191 -
    0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,  // 192-207  - псевдографика
    0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,  // 208-223 -
    2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,  // рстуфхцчшщъыьэюя
    2,2,2,2,2,2,2,2,0,0,0,0,0,0,0,0   // ЙО, йо, 6 украинских букв, затем всякая фигня
#endif
};

#define char_class(c)   (char_class_table[(unsigned)(c)])


// ВСПОМОГАТЕЛЬНЫЕ ПРОЦЕДУРЫ ***************************************************************

// Структура, хранящая информацию об одном слове из словаря
struct Word
{
    unsigned len;     // длина слова
    byte    *ptr;     // начало слова
union {
struct {              // Эти значения используются при построении и очистке словаря:
    unsigned hash;    //   хеш слова, используется для поиска его счётчика в cnt[]
    unsigned hash0;   //   хеш родительского слова
};
struct {              // Эти значения используются после очистки словаря:
    count_t  count;   //   количество использований этого слова
    byte chr, chr2;   //   байт(ы), которыми будет кодироваться это слово
};};
#ifdef DEBUG
    count_t use_count;  // количество использований этого слова при кодировании
#endif
};

// Массив слов, найденных в тексте
Word *FirstWord;

// Адрес первого свободного элемента в FirstWord
Word *NextWord;

// Адрес конца словаря. На следующих шагах алгоритма отражает конец используемой части таблицы
Word *LastWord;

// Добавить очередное слово в словарь
inline void AddWord (byte *ptr, unsigned len, unsigned hash, unsigned hash0)
{
    NextWord->ptr   = ptr;
    NextWord->len   = len;
    NextWord->hash  = hash;
    NextWord->hash0 = hash0;
    NextWord++;
}

// Возвращает длину совпадающего префикса двух слов
int common_prefix_length (Word *a, Word *b)
{
    int len = mymin (a->len, b->len), i;
    for (i=0; i<len && a->ptr[i] == b->ptr[i]; i++);
    return i;
}

// Функция сравнения для сортировки слов по убыванию частоты
int __cdecl count_desc_order (const Word *a, const Word *b)   { return b->count - a->count; }

#ifdef DEBUG
// Функция сравнения для сортировки слов по убыванию частоты актуального использования
int __cdecl use_count_desc_order (const Word *a, const Word *b)   { return b->use_count - a->use_count; }
#endif

// Функция лексикографического сравнения слов. Слово, являющееся префиксом другого, считается меньшим
int lexicographical_order (const Word *a, const Word *b)
{
  unsigned alen = a->len, blen = b->len;
  int cmp = memcmp (a->ptr, b->ptr, mymin(alen,blen));
  return cmp? cmp : alen-blen;
}



// Этот байт используется как префикс для тех символов, которые стали жертвой слов
byte PREFIX_FOR_WEAK_CHARS;



// Статистика символов
struct char_stats {
    byte     chr;     // сам символ
    count_t  count;   // счётчик его частоты
};

// Функция сравнения для сортировки символов по возрастанию частот
int char_count_asc_order (const char_stats *a, const char_stats *b)   { return a->count - b->count; }



// Пересчитать хеш слова с добавлением очередной буквы: hash("ab"),'c' -> hash("abc")
inline unsigned update_hash (unsigned hash, byte c)
{
    return hash*137 + c + 219;
}

// Рехеширование, используемое при обнаружении коллизий
inline unsigned rehash (unsigned hash, byte c)
{
    return hash + c*256 + 317;
}



// ПЕРВАЯ ЧАСТЬ УПАКОВКИ: ПОСТРОЕНИЕ СЛОВАРЯ ****************************************************

// Хеш-таблица, используемая при построении словаря
struct stats {
    scnt_t  count;   // счётчик частоты
    hash0_t hash0;   // хеш родительского слова
}
*scan_hash;

// Найти слот в хеш-массиве, хранящий информацию о слове hash c родителем phash
#define SEARCH_IN_HASH(hash,phash,c,x,on_found,on_long_hash_chain)                                                       \
{                                                                                                                        \
    hash0_t h; int n=13; debug ((used_hash1[n]++, depth_cnt[p+x-p0]++));                                                 \
    debug (verbose>3 && printf( "Hash   %08x %08x %08x c=%02x %d\n", phash, hash, scan_hash[hash&mask].hash0, c, x));    \
    while ((h = scan_hash[hash&mask].hash0) != 0) {                                                                      \
        if (h == (hash0_t)phash)   goto on_found;  /* Хеш-слот, соответствующий слову [p0..p), найден */                 \
        debug (used_hash1[n-1]++);                                                                                       \
        if (--n == 0)  goto on_long_hash_chain;  /* Защита от зацикливания и просто слишком длинных хеш-цепочек */       \
        hash = rehash (hash, c);                                                                                         \
        debug (verbose>3 && printf( "Rehash %08x %08x %08x\n", phash, hash, scan_hash[hash&mask].hash0));                \
    }                                                                                                                    \
}

// Добавить слово [p0,p) с хешем hash и родителем phash в словарь и хеш-таблицу
#define ADDWORD(hash,phash)                                                                                        \
{                                                                                                                  \
    unsigned len = p-p0;                                                                                           \
    debug (verbose>2 && printf( "AddWord '%.*s' len=%d %08x %08x\n", len, p0, len, hash, phash));                  \
                                                                                                                   \
    /* Проверка переполнения словаря, "переполнения" слова и обнуления младших 16 бит хеша */                      \
    if (NextWord<LastWord && len<=MAX_WORD_LEN && (hash0_t)hash!=0) {                                              \
        unsigned h = hash&mask, ph = phash&mask;                                                                   \
        scan_hash[ph].count++;                                                                                     \
        scan_hash[h].count = 1;                                                                                    \
        scan_hash[h].hash0 = phash;                                                                                \
        AddWord (p0,len,h,ph);                                                                                     \
        debug (addword_cnt[len]++);                                                                                \
    }                                                                                                              \
}

#define WORD_STEP 4

// время работы различных вариантов phase1
//         old d:1 e:4 e-h2 f-h1 f:WS8 WS4 WS2 WS1 f3:WS4 WS1 g:WS1   g4  WS1
//ghc-src  2.7 2.0 2.0 2.2  2.4    2.3 2.1 2.2 2.7    1.9 2.2   2.1   1.8 1.9
//javadoc  6.1 5.0 2.9 3.0  3.3    2.8 3.0 4.0 6.5    2.8 5.3   5.1   2.4 3.7

// Первый проход - создание списка слов через AddWord и подсчёт их частот в хеш-таблице scan_hash.
// Между делом также производится подсчёт частот байтов во входных данных.
//
// Словарь растёт динамически в lzw-стиле: если в нём уже есть слово "abc"
// с MIN_VISITS_TO_HAVE_SON(length) количеством повторов, то в словарь заносится слово "abcd".
// Для поиска слова максимальной длины, начинающегося с текущей позиции во входном потоке,
// и уже занесённого в словарь, используется trie (дерево цифрового поиска),
// отображаемое в хеш-таблицу с закрытым хешированием scan_hash. Новое значение хеша
// при добавлении к слову символа вычисляется функцией update_hash.
//
// Поиск ведётся спекулятивно - вместо того, чтобы последовательно проверять слова длины 2, 3, 4
// и т.д., мы ищем предполагаемый конец слова, тестируя предикат ALLOW_TO_EXTEND_WORD.
// Пока он возвращает истину, мы проверяем только каждое WORD_STEP = 4-е слово
// (с длинами 2, 6, 10...). И только убедившись, что очередного проверяемого слова не оказалось
// в словаре, мы начинаем проверять слова меньшей длины (например, 2, 6, 10 - не найдено, 7, 8, 9;
// или другой сценарий: 2, 6, 9 - ! ALLOW_TO_EXTEND_WORD, 7, 8).
// Отключение спекулятивности поиска (WORD_STEP = 1) чуть-чуть улучшит сжатие ценой скорости
int phase1 (byte *buf, unsigned bufsize)
{
    // Максимально допустимое количество слов - 1/32 от объёма входных данных
    unsigned max_words = roundup_to_power_of (mymax(bufsize/32,32768), 2);
    FirstWord = (Word*) BigAlloc (max_words * sizeof (Word));
    LastWord = FirstWord+max_words;
    NextWord = FirstWord;

    // Для уменьшения числа коллизий объём хеша - вдвое больше максимального количества слов
    unsigned scanhash_size = max_words*2, mask = scanhash_size-1;
    scan_hash = (stats*) BigAlloc (scanhash_size * sizeof (stats));
    memset (scan_hash, 0, scanhash_size * sizeof (stats));
    // Не дадим словам использовать нулевой элемент хеша, поскольку это сделает невозможным поиск их детей
    // То же самое применимо к элементам хеша, чей индекс кратен 2^16 (при 16 битах, используемых для хранения значения хеш-функции в scan_hash)
    for (int i=0; i<scanhash_size; i+= 1<<(sizeof(hash0_t)*8)) {
        scan_hash[i].hash0 = 1;
        if (sizeof(hash0_t) >= sizeof(int))    break;   // Без этой проверки мы можем зациклиться на нулевом элементе ;)
    }

    byte *p = buf,                  // указатель на следующий обрабатываемый символ
         *endbuf = buf+bufsize-WORD_STEP-1;  // конец обрабатываемой части буфера

    do {
        byte *p0 = p;             // указатель начала текущего обрабатываемого слова
        unsigned c1 = *p++;  unsigned c = *p;  // предпоследний и последний символы
        unsigned hash0;           // здесь будет храниться хеш слова [p0,p)
        // Если в этой позиции нельзя сформировать даже двухбайтовое слово - сразу ставим паруса и отчаливаем :)
        if (! ALLOW_TO_EXTEND_WORD (c1, c))   {debug (badword_cnt[1]++); goto end;}
        p++;
        hash0 = (c1 << 8) + c + 16;

        // Найдём 2-байтовое слово
        SEARCH_IN_HASH (hash0, hash0, hash0, 0, found2, end);
        // Слово не найдено, добавим его в словарь
        ADDWORD (hash0, hash0);  goto end;

    found2:
        // Найти минимальное слово, начинающееся с текущей позиции,
        // и ещё не занесённое в словарь (делаем ADDWORD),
        // или имеющее слишком малый счётчик (делаем INCWORD)
        // Маленькое замечание - в комментариях внутри этого цикла n-байтовым словом я
        // называю слово длины p-p0+n. Это просто для краткости. И вообще,
        // каждый шаг этого цикла имеет дело со словами длины от p-p0 до p-p0+WORD_STEP-1
        do {
            unsigned hash=hash0, hash1=hash0; int i;

            // Сначала первый проход - расширяем слово пока нам встречаются приличные символы :)
            // Обновляем значение хеш-функции, но пока не предпринимаем никаких
            // попыток проверить, что эти слова действительно есть в словаре.
            // Можете назвать это в мою честь спекулятивным поиском :)
            // Такой подход оказывается выгоднее, поскольку на практике слова
            // как правило ограничены именно несовместимыми символами, а не чем-либо ещё
            for (i=0; i<WORD_STEP; i++) {
                c1 = c; c = p[i];
                if (! ALLOW_TO_EXTEND_WORD (c1,c))  goto search_max;
                hash = update_hash (hash1=hash, c);
            }

            // Звёзды нам благоприятствуют - слово максимально возможной длины состоит сплошь из хороших и отличных букв
            debug (matrix_cnt [p-p0+i] [0] ++);
            SEARCH_IN_HASH (hash, hash1, c1, i, next_cycle, found_max);  goto search_less;
    next_cycle: // Слово максимальной длины (WORD_STEP) найдено - надо поискать слова ещё длиннее!
            { p += i;
              scnt_t *counter_p = &scan_hash[hash&mask].count;
              int counter = *counter_p;   // счётчик этого слова
              if (counter>=MIN_VISITS_TO_HAVE_SON(p-p0))  {hash0=hash; continue;}
              if (counter>0) {*counter_p = counter+1; debug (increment_cnt[p-p0]++); break;}
              ADDWORD (hash, hash1);
              break;
            }

    search_max:  // Ищем слово длины i - максимально возможной (дальше идут непотребные символы)
            if (i>0)  {
                debug (matrix_cnt [p-p0+i] [0] ++);
                SEARCH_IN_HASH (hash, hash1, c1, i, found_max, found_max);
                goto search_less;
            }
    found_max:  // Слово найдено, но расширить его мы не можем из-за того, что дальше идут
                // негодные символы (млм мы наткнулись на слишком длинную цепочку в хеше)
                // - остаётся только увеличить его счётчик
            { p += i;
              scnt_t *counter_p = &scan_hash[hash&mask].count;
              int counter = *counter_p;
              if (counter<=0)  {ADDWORD (hash, hash1); break;}
              if (counter < SCNT_MAX-1)  *counter_p = counter+1;
              debug (badword_cnt[p-p0]++);
              break;
            }

    search_less: int maxi = i;  // Индекс последнего заполненного элемента в массиве hash
            // А теперь второй проход - установив максимальную теоретически возможную длину слова выше
            // (поскольку дальше уже идут несовместимые символы), но удостоверившись, что столь длинное
            // слово в словаре отсутствует, мы перебираем слова с начала, отыскивая самое длинное, которое
            // всё же присутствует в словаре
            unsigned h0=hash0,h1,h2,h3=hash0;
            for (i=1; i<maxi; i++) {
                byte c = *p++;
                h1 = h2 = update_hash (h0, c);
                debug (matrix_cnt [p-i+maxi-p0] [maxi-i] ++);
                SEARCH_IN_HASH (h2, h0, c, i, next, found_max);  p--; goto found;
                next: h0 = h1; h3 = h2;
            }
            h2 = hash;

    found:  // Слово длины i есть в словаре, i+1 - нет. 0<=i<WORD_STEP
            scnt_t *counter_p = &scan_hash[h3&mask].count;
            int counter = *counter_p;   // счётчик этого слова
            if (counter>=MIN_VISITS_TO_HAVE_SON(p-p0)) {
                // У найденного слова уже достаточно посещений. Мы должны добавить в словарь
                // новое слово, расширенное на один символ по сравнению с этим
                p++;  ADDWORD (h2, h0);
            } else {
                // Счётчик посещений этого слова ещё слишком мал - просто увеличим его
                *counter_p = counter+1;
                debug (increment_cnt[p-p0]++);
            }
            break;
        } while (p < endbuf);

    end:while (p0<p)  char_counts[*p0++]++;  // подсчёт частот байтов во входных данных

    } while (p < endbuf);

    while (p<buf+bufsize)  char_counts[*p++]++;

    LastWord = NextWord;  // теперь это - конец словаря

#ifdef DEBUG
    // Печать отладочной статистики
    debug (verbose>1 && printf( "                 depth                                 increment addword badword\n") );
    int dc=0, ic=0, ac=0, bc=0;
    for (int n=0; n<=MAX_WORD_LEN+1; n++) {
       dc += depth_cnt[n];  ic += increment_cnt[n];  ac += addword_cnt[n];  bc += badword_cnt[n];
       for (int m=0; m<=MAX_WORD_LEN+1; m++)  mc[m] += matrix_cnt[n][m]; }
    debug (verbose>1 && printf( "Summary     : %8d %7d %7d %7d %7d %9d %7d %7d\n", dc, mc[0], mc[1], mc[2], mc[3], ic, ac, bc) );
    for (int n=0; n<=MAX_WORD_LEN+1; n++)
    debug (verbose>1 && printf( "Word len %3d: %8d %7d %7d %7d %7d %9d %7d %7d\n", n, depth_cnt[n], matrix_cnt[n][0], matrix_cnt[n][1], matrix_cnt[n][2], matrix_cnt[n][3], increment_cnt[n], addword_cnt[n], badword_cnt[n]) );
    debug (verbose>1 && printf( " Hash collisions:") );
    for (int n=13; n>=0; --n)   debug (verbose>1 && printf( " %d", used_hash1[n]) );
    debug (verbose>1 && printf( "\n") );
    debug (verbose>0 && printf( " Collected words: %d         ", LastWord-FirstWord) );
#endif

    return 0;  // All right
}


// Пройти по списку слов сначала из начала в конец, отдавая счётчики однодетных родителей их детям.
// А затем с конца в начало (чтобы сначала обработать производные, более длинные слова)
// отдавая счётчики слишком редких слов их родителям. Счётчики извлекаются из всё той же хеш-таблицы
int phase2 (unsigned bufsize, int MinLargeCnt, int MinMediumCnt, int MinSmallCnt, int MinRatio)
{
    debug (int PromotedWords=0);  // Счётчик для вывода статистики при отладке

    // Отдать счётчики слов, имеющих только одного ребёнка, их детям.
    // Такие слова появляются в словаре только из-за пошаговой процедуры его построения
    stat2 ("Отдать счётчики однодетных родителей их детям");
    for (Word *p=FirstWord; p<LastWord; p++) {
        // Перенести данные о текущем слове в локальные переменные
        debug (byte *ptr = p->ptr);  unsigned len = p->len, hash = p->hash, hash0 = p->hash0;
        // Счётчики текущего слова и его родителя
        count_t cnt = scan_hash[hash].count, cnt0 = scan_hash[hash0].count;

        // Если слово-родитель имеет стандартную минимальную частоту, свидетельствующую о том,
        // что у него только один "ребёнок", или отрицательную частоту, свидетельствующую о том же самом
        // то отдадим частоту родительского слова текущему
        if (cnt0 == MIN_VISITS_TO_HAVE_SON(len-1)+1 || cnt0 < 0) {
            scan_hash[hash0].count = 0;              // исключим слово-родителя
            count_t sumcnt = mymin(abs(cnt) + abs(cnt0), SCNT_MAX);  // отдав его частоту текущему слову
            // Если текущее слово имеет только одного ребёнка (об этом можно судить по его счётчику,
            // равному минимальному значению, при котором можно завести ребёнка, плюс 1),
            // то мы пометим его счётчик отрицательным знаком, чтобы его единственный ребёнок
            // дальше забрал себе эту частоту
            scan_hash[hash].count  =  (cnt == MIN_VISITS_TO_HAVE_SON(len)+1) ?  -sumcnt : sumcnt;
            debug (verbose>2 && printf( "Promoted '%.*s' %d -> %d (%d)\n", len, ptr, cnt, scan_hash[hash].count, cnt0));
            debug (PromotedWords++);
        }
    }
    debug (verbose>0 && printf( " Promoted words: %d\n", PromotedWords));

    // Отдать счётчики плохих, негодных слов их родителям
    stat2 ("Отдать счётчики плохих детей их родителям");  Word *q=LastWord;
    for (Word *p=LastWord; --p>=FirstWord;) {
        // Перенести данные о текущем слове в локальные переменные
        debug (byte *ptr = p->ptr; unsigned len = p->len);  unsigned hash = p->hash, hash0 = p->hash0;
        // Счётчики текущего слова и его родителя
        count_t cnt = abs(scan_hash[hash].count), cnt0 = abs(scan_hash[hash0].count);
        debug (verbose>1 && cnt >= 2000 && printf( "INTERESTING_WORD '%.*s' %d (%d)\n", len, ptr, cnt, cnt0));

        // Если текущее слово имеет достаточное количество повторений/длину
        if (GOOD_WORD (cnt, cnt0, len)) {
            p->count = cnt;   // избавимся от возможного знака минус в счётчике
            *--q = *p;        // и перенесём слово в список выживших после страшной чистки :)
            debug (verbose>1 && printf( "GOOD_WORD '%.*s' %d (%d)\n", len, ptr, cnt, cnt0));
        } else {
            scan_hash[hash0].count = mymin(cnt+cnt0,SCNT_MAX);  // отдав его частоту слову-родителю
            debug (verbose>2 && printf( "BadWord '%.*s' %d (%d)\n", len, ptr, cnt, cnt0));
        }
    }
    BigFreeAndNil (scan_hash);  // Скрипач больше не нужен :)

    // Перенесём выжившие слова в начало массива FirstWord и уменьшим его, чтобы оставить только их
    int good_words = LastWord-q;
    memmove (FirstWord, q, good_words*sizeof(Word));  LastWord = FirstWord + good_words;
    //FirstWord = (Word*) realloc (FirstWord, good_words*sizeof(Word));  -- impossible due to use of BigAlloc
    debug (verbose>0 && printf( " Good words: %d                ", good_words) );

    return good_words>0? 0 : -1;  // All right if there is at least one good word
}


// Теперь мы имеем список наиболее употребительных слов с их частотами и частоты отдельных символов
// во входном тексте, которые также были собраны на первом этапе.
// Отсортировав оба списка по частотам, мы легко узнаем, какие символы можно задействовать для
// кодирования слов и какие слова будут кодироваться одно-, и какие двух-буквенными сокращениями.
// Значение *nodes, возвращаемое функцией - количество слов в начале массива FirstWord, которые должны
// быть закодированы одним байтом. LastWord-FirstWord определяет общее количество кодируемых слов
int phase3 (int MinWeakChars, int *nodes)
{
    // Отсортируем выжившие слова в порядке убывания частоты употребления
    qsort( FirstWord, LastWord-FirstWord, sizeof(Word),
           (int (__cdecl *)(const void*, const void*)) count_desc_order);

    // Отсортируем символы в порядке возрастания частоты употребления
    char_stats chars[UCHAR_MAX+1];
    for (int c=0; c<=UCHAR_MAX; c++) {
        chars[c].chr   = c;
        chars[c].count = char_counts[c];
        debug (verbose>1 && printf( "Freq %02x: %d\n", c, char_counts[c]) );
    }
    qsort( chars, UCHAR_MAX+1, sizeof(char_stats),
           (int (__cdecl *)(const void*, const void*)) char_count_asc_order);

    // Определим, сколько слов можно закодировать однобайтовыми символами
    int n=0;
    for (Word *p=FirstWord; p<LastWord && n<=UCHAR_MAX; p++, n++) {
        // Ищем первый символ с достаточно большой частотой, который нет смысла отдавать под кодирование слов
        debug (verbose>1 && printf("Ccnt=%d, Wcnt=%d\n", chars[n].count, p->count) );
        if (chars[n].count >= p->count)  break;
        char_counts[chars[n].chr] = 0;  // делаем этот символ "условно-свободным", чтоб он мог использоваться для кодирования слов
    }
    if (n<=MinWeakChars)  return -1;  // Если для кодирования слов удалось высвободить так мало символов, то этот файл - скорей всего бинарный

    // Отдадим последний из освобождённых символов под префикс для тех символов, чьи коды были отданы словам
    byte c = chars[--n].chr;
    char_counts[c] = 1;
    PREFIX_FOR_WEAK_CHARS = c;

    // Теперь мы знаем, сколько символов можно использовать для кодирования слов
    int avail_count = n;
    debug (verbose>0 && printf( " Weak chars: %d\n", avail_count) );

    // Возвратим количество слов, которые должны кодироваться одним байтом
    // Оно может быть меньше avail_count, поскольку мы используем эти коды и для
    // двухбайтовых слов. Это не совсем правильно, но с практической точки зрения
    // большого значения иметь не должно
    int word_count = LastWord-FirstWord;
    *nodes = (use_plain_dictionary || word_count<=avail_count)
                ? mymin (word_count, avail_count)                   // если должны использоваться только однобайтовые слова или для всех найденных слов места достаточно (хм, маловероятно ;)
                : mymax (avail_count - (word_count+259)/256, 0);    // если используются и такие, и такие слова
    return 0;  // All right
}


// Назначить коды оставшимся в словаре словам
int phase4 (int nodes)
{
    Word *p = FirstWord;           // указатель текущего обрабатываемого слова
    Word *TwoByteWords = p+nodes;  // первое слово, которому нужно дать двухбайтовый код

    // Отсортируем отдельно одно- и двухбайтовые слова в лексикографическом порядке
    // для улучшения степени сжатия самого словаря
    qsort( FirstWord, nodes, sizeof(Word),
           (int (__cdecl *)(const void*, const void*)) lexicographical_order);  // NB! Быстрее будет patricia-qsort
    qsort( TwoByteWords, LastWord-FirstWord-nodes, sizeof(Word),
           (int (__cdecl *)(const void*, const void*)) lexicographical_order);  // NB! Быстрее будет patricia-qsort

    // Первый цикл - назначить однобайтовые коды наиболее полезным словам
    int c;
    for( c=0; c<=UCHAR_MAX && p<TwoByteWords; c++ ) {
        if( !char_counts[c] && c!=RESERVED_CHAR ) {
            debug (verbose>1 && printf( "Word1 %02x '%.*s' %d\n", c, p->len, p->ptr, p->count));
            p->chr = (byte)c;
            p->chr2 = RESERVED_CHAR;  p++;
        }
    }

    // Второй цикл - назначить двухбайтовые коды всем остальным словам
    for (; c<=UCHAR_MAX && p<LastWord; c++) {
        if (char_counts[c] || c==RESERVED_CHAR)  continue;    // Этот символ нельзя использовать - он встречается в тексте или является зарезервированным
        for (int c2=0; c2<=UCHAR_MAX && p<LastWord; c2++) {
            if (c2 == RESERVED_CHAR)  continue;               // Этот символ нельзя использовать
            while (p<LastWord && !GOOD_2BYTE_WORD(p->len, p->count))  (*p++).count=0;   // Пропустим те слова, которые имело смысл кодировать только однобайтными кодами
            if (p<LastWord) {
                debug (verbose>1 && printf( "Word2 %02x %02x '%.*s' %d\n", c, c2, p->len, p->ptr, p->count));
                p->chr = (byte)c;
                p->chr2 = (byte)c2;  p++;
            }
        }
    }
    LastWord = p;  // Теперь LastWord указывает на последнее закодированное слово

    // А сейчас можно зафигачить однобайтовые коды ещё нескольким словам (за счёт тех сотен очень маленьких слов, которые отказались от двухбайтовых кодов - см. !GOOD_2BYTE_WORD выше)
    for ( p=TwoByteWords; c<=UCHAR_MAX && p<LastWord; c++) {
        if (!char_counts[c] && c!=RESERVED_CHAR) {
            debug (verbose>1 && printf( "Word1 %02x '%.*s' %d\n", c, p->len, p->ptr, p->count));
            p->chr = (byte)c;
            p->chr2 = RESERVED_CHAR;  p++;
        }
    }
#ifdef DEBUG
    int nodes1 = p-TwoByteWords;  // количество доп. однобайтовых слов для статистики
    int sumcnt1=0, sumcnt2=0;     // суммарная частота одно/двухбайтовых слов для статистики
#endif

    // Избавимся от слов, так и не получивших коды:
    //   Для этого отсортируем слова в порядке убывания частоты
    qsort( FirstWord, LastWord-FirstWord, sizeof(Word),
           (int (__cdecl *)(const void*, const void*)) count_desc_order);
    //   И найдём последнее слово с ненулевой частотой
    for (p = FirstWord; p<LastWord && p->count; p++) {
        debug (p->chr2 == RESERVED_CHAR?  sumcnt1 += p->count : sumcnt2 += p->count);
    }
    LastWord = p;   // Вот теперь все слова от FirstWord до LastWord имеют коды
    debug (verbose>0 && printf( " Final words: %d = %d+%d + %d\n", LastWord-FirstWord, nodes, nodes1, LastWord-FirstWord-nodes-nodes1) );
    debug (verbose>0 && printf( " Counts: %dw1 + %dw2\n", sumcnt1, sumcnt2) );
    return 0;  // All right
}


#define put_byte(c)      (*outptr++ = (c))

#endif // FREEARC_DECOMPRESS_ONLY
#define dict2(i,j)       (dict2_var [i*(UCHAR_MAX+1) + j])
#define dict_len(i)      (dict[i]? dict[i]->len : 0)
#define dict_ptr(i)      (dict[i]->ptr)
#define dict2_len(i,j)   (dict2(i,j)? dict2(i,j)->len : 0)
#define dict2_ptr(i,j)   (dict2(i,j)->ptr)
#ifndef FREEARC_DECOMPRESS_ONLY

// Выделить память под выходной буфер и записать словарь в его начало
int phase5 (byte **outbuf, unsigned *outsize, unsigned bufsize)
{
    // Если окажется, что кодирование невозможно, в эту переменную будет занесён код ошибки
    int retcode = 0;

    // Выделим память для матрицы слов
    Word **dict      = (Word**) calloc ( UCHAR_MAX+1,                sizeof (Word*));
    Word **dict2_var = (Word**) calloc ((UCHAR_MAX+1)*(UCHAR_MAX+1), sizeof (Word*));
    byte *char_in_use = (byte*) calloc ( UCHAR_MAX+1,                sizeof (byte));

    // Заполнить матрицу слов для упрощения вывода словаря в выходной поток
    Word USE_DICT2_WORD[1]; USE_DICT2_WORD->len = USE_DICT2;
    for (Word *p = FirstWord; p<LastWord; p++) {
        if (p->chr2 == RESERVED_CHAR) {
            dict[p->chr] = p;               // Занести слово в словарь первого уровня
        } else {
            dict2(p->chr,p->chr2) = p;      // Занести слово в словарь второго уровня
            dict[p->chr] = USE_DICT2_WORD;  // Отметить в словаре первого уровня использование этого кода для двухбайтовых слов
            // Отметим в массиве char_in_use все символы, используемые в этом слове
            for (int c=0; c < p->len; c++) {
                char_in_use[ p->ptr[c] ] = 1;
            }
        }
    }
    // Найдём не встречающийся ни в одном слове символ. Он будет использоваться
    // как разделитель при кодировании слов
    byte word_sep;
    for (int c=UCHAR_MAX; c>=0; c--) {
        if (!char_in_use[c])  {word_sep=c; goto found;}
    }
    retcode = -1;  // В словах задействованы все символы, что делает невозможным кодирование словаря
    goto done;
found:
    {
    // Выделить под упакованные данные размер входного файла
    // плюс 200 кб - должно хватить при любом раскладе :)
    *outbuf = (byte*) BigAlloc (bufsize+200000);
    byte *outptr = *outbuf;        // текущий указатель в выходном буфере

    // Вывести словарь в выходной поток в 5 приёмов:
    //   длины однобайтовых слов, длины префиксов двухбайтовых слов,
    //   содержимое однобайтовых слов, остатки содержимого двухбайтовых слов,
    //   спецсимвол используемый как префикс для кодирования "украденных" символов
    // При этом длины выводятся сразу для всех 256 возможных кодов (или субкодов),
    //   где 0 означает неиспользуемые коды, а 1 (USE_DICT2) - коды начала двухбайтовых слов
    for (int i=0; i<=UCHAR_MAX; i++) {
        put_byte (dict_len(i));
    }
    debug (verbose>1 && printf( "  Dict1 lenghts: %d\n", outptr-*outbuf) );
    Word *prev_word = NULL;
    for (int i=0; i<=UCHAR_MAX; i++) {
        if (dict[i] == USE_DICT2_WORD) {
            for (int j=0; j<=UCHAR_MAX; j++) {
                unsigned n = 0;
                if (dict2(i,j) && prev_word) {
                    n = common_prefix_length (dict2(i,j), prev_word);
                }
                put_byte (n);
                prev_word = dict2(i,j);
            }
        }
    }
    debug (verbose>1 && printf( "  Dict2 prefixes: %d\n", outptr-*outbuf) );
    for (int i=0; i<=UCHAR_MAX; i++) {
        if (dict[i] == USE_DICT2_WORD)  continue;
        for (int c=0; c < dict_len(i); c++) {
            put_byte (dict_ptr(i)[c]);
        }
    }
    debug (verbose>1 && printf( "  Dict1 strings: %d\n", outptr-*outbuf) );
    prev_word = NULL;
    put_byte (word_sep);  // Сообщим декодеру разделитель слов
    for (int i=0; i<=UCHAR_MAX; i++) {
        if (dict[i] == USE_DICT2_WORD) {
            for (int j=0; j<=UCHAR_MAX; j++) {
                int n = 0;
                if (dict2(i,j) && prev_word) {
                    n = common_prefix_length (dict2(i,j), prev_word);
                }
                // Выведем текст слова минус префикс, который должен быть скопирован из предыдущего слова
                for (int c=n; c < dict2_len(i,j); c++) {
                    put_byte (dict2_ptr(i,j)[c]);
                }
                put_byte (word_sep);  // Сообщим декодеру, что очередное слово закончилось
                prev_word = dict2(i,j);
            }
        }
    }
    debug (verbose>1 && printf( "  Dict2 strings: %d\n", outptr-*outbuf) );

    // Выведем код символа, используемого как префикс для бездомных символов
    put_byte (PREFIX_FOR_WEAK_CHARS);
    // Статистика - общий размер словаря
    debug (verbose==1 && printf( " Dictionary: %d bytes  ", outptr-*outbuf) );

    // Возвратить длину закодированного словаря
    *outsize = outptr - *outbuf;
    }
    // Освободить внутренние массивы и выйти с кодом возможной ошибки
done:
    FreeAndNil (dict);
    FreeAndNil (dict2_var);
    FreeAndNil (char_in_use);
    return retcode;
}



// УПАКОВКА: КОДИРОВАНИЕ ПО ВЫШЕПОСТРОЕННОМУ СЛОВАРЮ **********************************************

// Макс. длина слова, которое можно сохранить непосредственно в структуре CodeWord
#define DIRECT_CHARS 12

// Структура, хранящая информацию о слове при кодировании
struct CodeWord
{
union {
    byte str[DIRECT_CHARS]; // содержимое слова если его длина <= 12
    byte *ptr;              // содержимое слова при длине > 12
};
    byte len;               // длина слова
    byte chr, chr2;         // байт(ы), которыми кодируется это слово
#ifdef DEBUG
    count_t count;          // количество использований этого слова при разборе текста
    count_t use_count;      // количество использований этого слова при кодировании
    Word   *orig_word;      // ссылка на оригинал слова, необходима для подсчёта общего количества его использований
#endif
};

unsigned  hashsize;
unsigned  hashmask;
CodeWord *codewords_hash;
ushort   *hashbits;
byte     *words_text;

// Строим небольшой хеш для всех используемых слов и их частичных начал.
// Этот хеш должен позволить нам просканировать текст до конца слова и определить его код.
// Поскольку, как правило, количество слов <10k и их суммарная длина <100k, то этот хеш должен
// полностью влезать в кеш CPU (размер хеша, куда происходит обращение для каждого байта
// входного текста = hashsize*2 = количество уникальных байт в словаре*8, округлённое вверх до степени двух;
// размер хеша, куда происходит однократное обращение для каждого слова = hashsize*16).
// Типично это 128-512 кб и 1-4 мб, соответственно; причём из второй таблицы используются всего 100-200 кб.
// Плюс ещё 10-50 кб для хранения текста слишком длинных слов (с длиной >12)
// Таким образом, кодирование должно обходиться кешем в 256-1024 кб, в зависимости от объёма словаря
int phase6 ()
{
    // Отсортируем слова в лексикографическом порядке для обеспечения создания
    // ссылок на родительские слова (скажем, если в словаре есть слова из 5 и 8 пробелов,
    // то индексы в хеше слов, соответствующие строкам из 6 и 7 пробелов, будут также содержать
    // информацию для кодирования 5-пробельного слова. Таким образом, при входной строке, например,
    // из 7 пробелов и табуляции, мы в последней записи из нашей хеш-цепочки (соответствующей
    // 7 пробелам) будем иметь ссылку на представление 5-пробельного слова)
    qsort( FirstWord, LastWord-FirstWord, sizeof(Word),
           (int (__cdecl *)(const void*, const void*)) lexicographical_order);  // NB! Быстрее будет patricia-qsort

    // Статистика: напечатать список слов в лексикографическом порядке
    for (Word *p = FirstWord; p<LastWord; p++) {
        debug (verbose>3 && printf( "SortedWord '%.*s' %d\n", p->len, p->ptr, p->count));
    }

    // Подсчитаем кол-во уникальных байт в словах. Это значение определяет общее количество записей,
    // которые будут созданы в хеш-таблице. Создадим хеш-таблицу как минимум вчетверо
    // большего размера для обеспечения высокой производительности поиска (шоб было <20% коллизий)
    unsigned unique_bytes = 0, words_len = 0;
    for (Word *p = FirstWord; p<LastWord; p++) {
        unique_bytes +=  p->len - (p==FirstWord? 0 : common_prefix_length (p, p-1));
        if (p->len > DIRECT_CHARS)  words_len += p->len;
    }
    hashsize = roundup_to_power_of (unique_bytes*4, 2);
    hashmask = hashsize-1;
    hashbits = (ushort*) calloc (hashsize, sizeof(ushort));
    codewords_hash = (CodeWord*) calloc (hashsize, sizeof(CodeWord));

    // Этот буфер будет использоваться для хранения текста длинных слов
    // (текст коротких слов заносится непосредственно в запись CodeWord)
    words_text = (byte*) malloc (words_len);
    byte *wordsptr = words_text;

    // Заполняем хеш-таблицу codewords_hash словами
    // и создаём хеш-цепочки в hashbits для побуквенного поиска этих слов
    for (Word *p = FirstWord; p<LastWord; p++) {
        unsigned hash = hashsize + p->ptr[0];
        CodeWord *longest_word_so_far = NULL;
        for (int i=1; i < p->len; i++) {
            byte c = p->ptr[i];
            unsigned hash0 = hash;
            hash = update_hash (hash, c);  int n=13;
            while (hashbits [hash & hashmask]  &&  hashbits[hash&hashmask] != (ushort)hash0) {
                if (--n == 0)  goto NextWord;  // Защита от зацикливания
                hash = rehash (hash, c);
            }
            hashbits [hash & hashmask] = hash0;

            // Размножение слов-префиксов текущего вдоль его хеш-цепочки
            CodeWord *newp = &codewords_hash [hash & hashmask];
            if (newp->len) {
                longest_word_so_far = newp;     // Запомним наиболее длинное so far слово, являющееся префиксом текущего
            } else if (longest_word_so_far) {
                *newp = *longest_word_so_far;   // Максимальное слово, которое можно закодировать в этой ситуации
                debug (newp->count = -newp->count);
            }
        }
        {   // Занесём слово в хеш-таблицу слов
            CodeWord *newp = &codewords_hash [hash & hashmask];
            newp->len  = p->len;
            newp->chr  = p->chr;
            newp->chr2 = p->chr2;
            // Текст слова или целиком запоминается в этой записи (если он не превышает 12 байт)
            // или переносится в words_text, куда мы делаем ссылку
            if (p->len <= DIRECT_CHARS)  memcpy (newp->str, p->ptr, p->len);
            else newp->ptr = wordsptr, memcpy (wordsptr, p->ptr, p->len), wordsptr += p->len;
            debug (newp->count = p->count);
            debug (newp->orig_word = p);
            debug (verbose>2 && printf( "RecordWord hash=%d len=%d '%.*s'\n", hash & hashmask, p->len, p->len, p->ptr));
        }
NextWord: ;
    }
    return 0;  // All right
}


// Найти в словаре самое длинное слово, с которого начинается текст, указываемый p0
// NB! Алгоритм FindWord должен в точности соответствовать алгоритму хеширования словаря в phase6,
// иначе мы рискуем не отыскать часть слов
inline CodeWord *FindWord (byte *p0, byte *endbuf)
{
    debug (verbose>3 && printf( "FindWord '%.50s'\n", p0));
    debug (FindWord_calls++);

    // Начав с двухбайтового слова из входного текста, мы увеличиваем его размер байт за байтом,
    // проверяя, что это слово всё ещё отмечено в хеше как входящее в словарь.
    // По окончанию этой процедуры hash0 содержит хеш самого длинного слова из входного текста,
    // имеющегося в словаре (если в словаре есть 5- и 8-пробельные слова, а входные данные
    // содержат только 7 пробелов, то hash0 будет хешем от 7 пробелов и по этому адресу в code_words
    // будет 5-пробельное слово - хошь-верь, хошь-нет)
    byte *p = p0;
    unsigned hash0 = hashsize + *p++;   if (p==endbuf)  return NULL;
    do {
        // Вычислим hash для слова [p0,p)
        byte c = *p++;
        unsigned hash = update_hash (hash0, c);

        // Цикл рехеширования. h=0 означает, что слот в хеш-таблице пуст,
        // то есть слова [p0..p) нет в словаре. h!=hash0 означает хеш-коллизию - слот не пуст,
        // но и занят не нами. Надо продолжить поиск путём вторичного рехеширования
        ushort h; int n=13;  debug (used_hash2[n]++);
        while ((h = hashbits [hash & hashmask]) != 0) {
            if (h == (ushort)hash0)   goto found;  // Хеш-слот, соответствующий слову [p0..p), найден
            hash = rehash (hash, c);
            debug (used_hash2[n-1]++);
            if (--n == 0)  break;  // Защита от зацикливания и просто слишком длинных хеш-цепочек
        }
        // Сюда мы попадаем если слова нет в словаре (или, очень редко,
        // при слишком длинной хеш-цепочке). Подходящий момент, чтобы выйти из цикла :)
        break;

found:  hash0 = hash;    // Слово найдено, переходим к поиску слова на один символ больше
    } while (p<endbuf);  // Но за конец буфера лучше всё же не выходить :D

    p--; // Слова, заканчивающегося на p, не нашлось, и теперь мы проверяем слово на один байт меньше
    CodeWord *word = &codewords_hash [hash0 & hashmask];
    int len = word->len;
    if (len==0 || len>endbuf-p)  return NULL;
    byte *ptr = len>DIRECT_CHARS? word->ptr : word->str;
    if (memcmp (p0, ptr, len) != 0)   return NULL;
    return word;
}

#define put_Byte(c)      (*outptr++ = (c))
#define put_Word(word)   (put_Byte (word->chr),  \
                         (word->chr2 != RESERVED_CHAR)  &&  put_Byte (word->chr2))

// Кодируем текст, используя вышепостроенный хеш
int phase7 (byte *buf, unsigned bufsize, byte *outbuf, unsigned *outsize)
{
    byte *p = buf,                // текущий указатель во входном буфере
         *endbuf = buf+bufsize,   // конец входного буфера
         *outptr = outbuf;        // текущий указатель в выходном буфере
    do {
        // Найти в словаре слово максимальной длины, начинающееся с текущей позиции
        CodeWord *word = FindWord (p,endbuf);
        // И уж если верное слово найдено..
        if (word) {
            put_Word (word);   // Вывести 1 или 2 байта - код найденного слова
            p += word->len;    // Пропустить слово во входном тексте
            debug (OutWord++);
            debug (word->use_count++);
            debug (word->orig_word->use_count++);
            debug (verbose>3 && printf( "OutWord %02x %02x '%.*s' %d\n", word->chr, word->chr2, word->len, word->len>DIRECT_CHARS? word->ptr : word->str, word->count));
        } else {
            // Слово не найдено - скопировать один символ из входного потока в выходной
            byte c = *p++;
            if (! char_counts[c] || c==PREFIX_FOR_WEAK_CHARS) {
                // Этому символу не повезло - он отдал свой код словам
                // и теперь вынужден идентифицировать себя спец-префиксом, бедняга
                put_Byte (PREFIX_FOR_WEAK_CHARS);
                debug (OutByte2++);
            }
            put_Byte (c);
            debug (OutByte++);
            debug (verbose>3 && printf( "OutByte %02x '%c'\n", c, c));
        }
    } while (p < endbuf);  // Повторять до конца входного буфера

#ifdef DEBUG
    // Отладочная статистика: частота использования слов при кодировании, в порядке её убывания
    debug (qsort( FirstWord, LastWord-FirstWord, sizeof(Word),
                  (int (__cdecl *)(const void*, const void*)) use_count_desc_order));
    int short_count=0, long_count=0;
    for (Word *p = FirstWord; p<LastWord; p++) {
        if (p->chr2 == RESERVED_CHAR)
            debug (verbose>1 && printf( "UsedWord1 %02x '%.*s' %d => %d\n", p->chr, p->len, p->ptr, p->count, p->use_count));
        else
            debug (verbose>1 && printf( "UsedWord2 %02x %02x '%.*s' %d => %d\n", p->chr, p->chr2, p->len, p->ptr, p->count, p->use_count));
        debug ((p->len > DIRECT_CHARS ?  long_count:short_count) += p->use_count);
    }
    debug (verbose>0 && printf( "    Usage: %ds+%dl\n", short_count, long_count) );
    debug (verbose>0 && printf( " Data: %d bytes = %d+%dc + %dw1 + %dw2\n", outptr-outbuf, OutByte, OutByte2, OutWord-(outptr-outbuf-FindWord_calls-OutByte2), outptr-outbuf-FindWord_calls-OutByte2) );
    debug (verbose>1 && printf( " Hash collisions (%d):", FindWord_calls) );
    for (int n=13; n>=0; --n)   debug (verbose>1 && printf( " %d", used_hash2[n]) );
#endif

    BigFreeAndNil (FirstWord);
    FreeAndNil (hashbits);
    FreeAndNil (codewords_hash);
    FreeAndNil (words_text);

    // Возвратить длину закодированного текста
    *outsize = outptr - outbuf;
    return 0;  // All right
}


// Вызвать заданную функцию и, если она возвратила код ошибки, - выйти из DictEncode(), освободив всю память
#define check(call)  { int code = call;                   \
                       if (code) {                        \
                           BigFreeAndNil (FirstWord);     \
                           BigFreeAndNil (scan_hash);     \
                           FreeAndNil (hashbits);         \
                           FreeAndNil (codewords_hash);   \
                           FreeAndNil (words_text);       \
                           BigFreeAndNil (*outbuf);       \
                           return code;                   \
                       }                                  \
                     }                                    \

// Упаковать входные данные buf[bufsize] и возвратить адрес выходного буфера и размер упакованных данных
int DictEncode (byte *buf, unsigned bufsize, byte **outbuf, unsigned *outsize, int MinWeakChars, int MinLargeCnt, int MinMediumCnt, int MinSmallCnt, int MinRatio)
{
    unsigned dictlen, datalen; int nodes; *outbuf = NULL;
    stat1 ("1. Построить словарь, найти частоты слов и отдельных символов");
    check (phase1 (buf, bufsize));
    stat1 ("2. Удалить из словаря слишком редкие слова");
    check (phase2 (bufsize,MinLargeCnt,MinMediumCnt,MinSmallCnt,MinRatio));
    stat1 ("3. Определить байты, которые можно использовать для кодирования слов");
    check (phase3 (MinWeakChars, &nodes));
    stat1 ("4. Назначить коды оставшимся в словаре словам");
    check (phase4 (nodes));
    stat1 ("5. Записать словарь в файл");
    check (phase5 (outbuf, &dictlen, bufsize));  // Выходной буфер создаётся именно здесь
    stat1 ("6. Создать хеш для быстрого поиска среди оставшихся слов");
    check (phase6 ());
    stat1 ("7. Закодировать текст");
    check (phase7 (buf, bufsize, *outbuf+dictlen, &datalen));
    // Возвратить размер и адрес выходного буфера
    *outsize = dictlen + datalen;
    //*outbuf  = (byte*) realloc (*outbuf, *outsize);  -- impossible due to use of BigAlloc
    return 0;  // All right
}

#endif // FREEARC_DECOMPRESS_ONLY


// РАСПАКОВКА *****************************************************************************

// Словарь-матрица, используемый для декодирования
struct dict_entry
{
    unsigned len;     // длина слова
    byte *ptr;        // начало слова
}
dict[UCHAR_MAX+1];

#define get_byte()       (*ptr++)
#define put_byte(c)      (*outptr++ = (c))
#define put_word(p,len)  (memcpy (outptr, (p), (len)), outptr += (len))

// Распаковать входные данные buf[bufsize] в outbuf и возвратить размер распакованных данных в *outsize
int DictDecode (byte *buf, unsigned bufsize, byte *outbuf, unsigned *outsize)
{
    int retcode = 0;
    byte *ptr = buf,
         *end = buf+bufsize,
         *outptr = outbuf;     // текущий указатель в выходном буфере

    // Словарь-матрица, используемый для декодирования 2-байтовых слов
    dict_entry *dict2_var = (dict_entry*) calloc ((UCHAR_MAX+1)*(UCHAR_MAX+1), sizeof (dict_entry));

    stat1 ("ЧТЕНИЕ СЛОВАРЯ");
    // Состоит из 5 циклов:
    //   1. Прочесть 256 байт - это длины всех слов, кодируемых одним байтом
    //        (0 означает, что этот байт слов не кодирует, 1 означает, что с этого байта начинаются коды 256 слов)
    //   2. Прочесть длины всех слов, кодируемых двумя байтами
    //        (256*n байт, где n - количество единиц, прочитанных на предыдущем этапе)
    //   3. Прочесть текст всех однобайтовых слов
    //   4. Прочесть текст всех двухбайтовых слов
    //   5. Создать псевдо-слова для декодирования тех символов, которые отдали свои коды словам
    int dictsize = 0, words2 = 0;
    for( int i=0; i<=UCHAR_MAX; i++ ) {
        dictsize += dict[i].len = get_byte();
    }
    for( int i=0; i<=UCHAR_MAX; i++ ) {
        if( dict[i].len==USE_DICT2 ) {
            for( int j=0; j<=UCHAR_MAX; j++ ) {
                dictsize += dict2(i,j).len = get_byte();
                words2++;
            }
        }
    }
    // Буфер для хранения текста слов (память для него сейчас выделяется на авось, но с большим запасом :)
    byte *words = (byte*) malloc (dictsize+UCHAR_MAX+1+words2*20+100000), *wordptr = words;
    for( int i=0; i<=UCHAR_MAX; i++ ) {
        if (dict[i].len == USE_DICT2)  continue;
        dict[i].ptr = wordptr;
        for( int k=0; k<dict[i].len; k++ ) {
            *wordptr++ = get_byte();
        }
    }
    {
    byte word_sep = get_byte();
    byte *prevptr = NULL;
    for( int i=0; i<=UCHAR_MAX; i++ ) {
        if( dict[i].len==USE_DICT2 ) {
            for( int j=0; j<=UCHAR_MAX; j++ ) {
                dict2(i,j).ptr = wordptr;
                // Скопируем начало слова из предыдущего
                for( int k=0; k<dict2(i,j).len; k++ ) {
                    if (prevptr==NULL)  {retcode = FREEARC_ERRCODE_BAD_COMPRESSED_DATA; goto done;}  // Ошибка во входных данных - копирование данных из предыдущего слова, которого нет :)
                    *wordptr++ = *prevptr++;
                }
                // И прочитаем остаток слова из входного потока
                for(;;) {
                    byte c = get_byte();
                    if (c==word_sep) break;
                    *wordptr++ = c;
                }
                dict2(i,j).len = wordptr - dict2(i,j).ptr;
                prevptr = dict2(i,j).ptr;
            }
        }
    }
    // Префикс, используемый для кодирования украденных символов
    byte prefix = get_byte();
    dict[prefix].len = USE_DICT2;
    // Создадим псевдо-слова, кодирующие украденные символы
    for (int j=0; j<=UCHAR_MAX; j++) {
        dict2(prefix,j).len = 1;
        dict2(prefix,j).ptr = wordptr;
        *wordptr++ = (byte)j;
    }


    stat1 ("ДЕКОДИРОВАТЬ ТЕКСТ, ИСПОЛЬЗУЯ ПРОЧИТАННЫЙ ВЫШЕ СЛОВАРЬ");
    while( ptr<end ) {
        byte c = get_byte();
        dict_entry &d = dict[c];

        // Если этот байт не кодирует никаких слов, то вывести его сам по себе
        if (d.len == 0) {
            put_byte(c);

        // Если этот байт - начало кода двухбайтового слова, то вывести это слово
        } else if( d.len == USE_DICT2 ) {
            byte c2 = get_byte();
            dict_entry &d = dict2(c,c2);
            put_word (d.ptr, d.len);

        // Иначе этот байт - начало кода однобайтового слова
        } else {
            put_word (d.ptr, d.len);
        }
    }
    }
done:
    FreeAndNil (words);
    FreeAndNil (dict2_var);

    // Записать длину декодированного текста и вернуть код (без)успешного завершения
    *outsize = outptr-outbuf;
    return retcode;
}



#ifndef DICT_LIBRARY
// ХОЗЧАСТЬ ************************************************************************
#include <windows.h>
#include <io.h>
#include "../Common.cpp"

// Вывести время выполнения очередного шага алгоритма и запомнить название следующего шага
void stat1 (char *nextmsg)
{
    if (! print_timings) return;
    stat2 (NULL);

    static char *msg = NULL;
    static LARGE_INTEGER Frequency, PerformanceCount0, PerformanceCountStart, PerformanceCountEnd;

    if (msg==NULL) {
        QueryPerformanceFrequency (&Frequency);
        QueryPerformanceCounter (&PerformanceCount0);
    } else {
        QueryPerformanceCounter (&PerformanceCountEnd);
        double seconds = double(PerformanceCountEnd.QuadPart - PerformanceCountStart.QuadPart)/Frequency.QuadPart;
        printf( "%s: %lf seconds\n", msg, seconds);
        if (nextmsg==NULL) {
            double seconds = double(PerformanceCountEnd.QuadPart - PerformanceCount0.QuadPart)/Frequency.QuadPart;
            printf( "Total %lf seconds\n", seconds);
        }
    }

    msg = nextmsg;
    QueryPerformanceCounter (&PerformanceCountStart);
}

// Аналогично stat1(), но для учёта времени выполнения подшагов алгоритма
void stat2 (char *nextmsg)
{
    if (! print_timings) return;

    static char *msg = NULL;
    static LARGE_INTEGER Frequency, PerformanceCountStart, PerformanceCountEnd;

    if (msg==NULL) {
        QueryPerformanceFrequency (&Frequency);
    } else {
        QueryPerformanceCounter (&PerformanceCountEnd);
        double seconds = double(PerformanceCountEnd.QuadPart - PerformanceCountStart.QuadPart)/Frequency.QuadPart;
        printf( "%s: %lf seconds\n", msg, seconds);
    }

    msg = nextmsg;
    QueryPerformanceCounter (&PerformanceCountStart);
}


// Разбор командной строки, чтение входных данных, вызов DictEncode/DictDecode, и запись выходных данных
int main (int argc, char **argv)
{
    // Распаковка вместо упаковки?
    int unpack = 0;

    // Параметры отбора слов, используемые по умолчанию
    int MinLargeCnt  = 2048;    // Минимальный "большой" счётчик
    int MinMediumCnt = 100;     // Минимальный "средний" счётчик
    int MinSmallCnt  = 50;      // Минимальный "маленький" счётчик
    int MinRatio     = 4;       // Минимальная "пропорция"

    if( argv[1] && argv[1][0] == '-' ) {
        char *p = argv[1]+1;
        while( *p ) {
            switch( tolower(*p) ) {
                case 'p':   MinLargeCnt=8192; MinMediumCnt=400; MinSmallCnt=100; MinRatio=4; break;
                case 'f':   MinLargeCnt=2048; MinMediumCnt=100; MinSmallCnt= 50; MinRatio=0; break;
                case '1':   use_plain_dictionary++;   break;
                case 'v':   verbose++;                break;
                case 't':   print_timings++;          break;
                case 'd':   unpack++;                 break;
                default :   printf( "\n Unknown option '%c'\n", *p);
                            exit(1);
            }
            p++;
        }
        argv++, argc--;
    }
    if (argc != 2  &&  argc != 3) {
        printf( "\n Usage: dict [-fp1vvvt] original-file [packed-file]");
        printf( "\n   -p  --  ppmd/ppmonstr-optimized compression");
        printf( "\n   -f  --  fast&dirty compression");
        printf( "\n   -1  --  use plain dictionary (only 1-byte codes)");
#ifdef DEBUG
        printf( "\n   -v  --  increment verbosity level (0 - default, 4 - maximum)");
#else
        printf( "\n   -v  --  verbosity level (you should recompile program with -DDEBUG to enable this option)");
#endif
        printf( "\n   -t  --  print operation timings");
        printf( "\n" );
        printf( "\n For decompress: dict -d[vvvt] packed-file [unpacked-file]");
        printf( "\n" );
        exit(1);
    }
    FILE *fin = fopen( argv[1], "rb" );
    if (fin == NULL) {
        printf( "\n Can't open %s for read\n", argv[1]);
        exit(2);
    }

    // Буфер, куда будут помещены входные данные, и их размер
    unsigned bufsize = filelength(fileno(fin));
    byte *buf = (byte*) malloc(bufsize);
    if (buf == NULL) {
        printf( "\n Can't alloc %u bytes\n", bufsize);
        exit(4);
    }

    // Прочитать входные данные
    unsigned bytes = file_read (fin, buf, bufsize);
    if (bytes != bufsize) {
        printf( "\n Can't read entire input file");
        exit(5);
    }
    debug (verbose>0 && printf( " Bytes read: %u\n", bufsize) );

    byte *outbuf; unsigned outsize; int ret;
    if (!unpack) {
        // Произвести кодирование и получить адрес выходного буфера и размер выходных данных
        ret = DictEncode (buf, bufsize, &outbuf, &outsize, 0, MinLargeCnt, MinMediumCnt, MinSmallCnt, MinRatio);
    } else {
        // Прочитать размер исходных данных из начала входного буфера
        // и использовать его для выделения памяти под выходной буфер
        outsize = *(unsigned*)buf;
        buf += sizeof(unsigned); bufsize -= sizeof(unsigned);
        outbuf = (byte*) malloc (outsize);
        // Произвести декодирование и получить размер выходных данных
        ret = DictDecode (buf, bufsize, outbuf, &outsize);
    }

    // Напечатать последнюю строку статистики
    stat1 (NULL);

    // Записать выходные данные, если всё ОК и был указан выходной файл
    if (!ret  &&  argc == 3) {
        FILE *fout = fopen( argv[2], "wb" );
        if (fout == NULL) {
            printf( "\n Can't open %s for write\n", argv[2]);
            exit(3);
        }
        if (!unpack)  file_write (fout, &bufsize, sizeof(bufsize));  // Добавим размер исходного файла в начало закодированных данных
        file_write (fout, outbuf, outsize);
    }

    return 0;
}

#endif

